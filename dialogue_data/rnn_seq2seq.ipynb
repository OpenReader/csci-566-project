{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "566 rnn-encoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuTMxecYNdX5",
        "outputId": "be788819-689a-4088-cbf0-4ee5bc65c167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['至大三上成绩单.pdf',\n",
              " 'BB9E1FA0@D3815A65.36CC2E59',\n",
              " '体检信息登记表.xlsx',\n",
              " 'resume.pdf',\n",
              " '申请信息表--推荐信表格.gdoc',\n",
              " '脚手架.rar',\n",
              " 'Colab Notebooks',\n",
              " 'Resume_xsy_2020_for job.pdf',\n",
              " 'extracted_dialogue.txt',\n",
              " 'CSCI544 presentation.gdoc',\n",
              " '4000_checkpoint (1).tar',\n",
              " '4000_checkpoint.tar',\n",
              " '566 rnn-encoder.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Xe7eXnMeIe",
        "outputId": "7e5f081a-23b1-4493-d2b3-1184bcd384b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "learning_rate = 0.1\n",
        "hidden_size = 256\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name=\"dict\"):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {EOS_token:\"EOS\",SOS_token:\"SOS\"}\n",
        "        self.n_words = 2  \n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)  \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size) \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1) \n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "def prepareData():\n",
        "    lang_dict = Lang()\n",
        "    sentences = []\n",
        "    max_length=0\n",
        "    with open(\"./extracted_dialogue.txt\",\"r\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\")\n",
        "        for i in range(0, len(lines), 2):\n",
        "            tmp = lines[i].strip()\n",
        "            if len(tmp.split(' ')) > 100:\n",
        "              continue\n",
        "            sentences.append(tmp)\n",
        "            lang_dict.addSentence(tmp)\n",
        "            if len(tmp.split(' ')) > max_length:\n",
        "                max_length = len(tmp.split(' '))\n",
        "    return lang_dict, sentences, max_length\n",
        "\n",
        "def getSentenceTensor(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden() \n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length+1, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  \n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach() \n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, lang_dict, max_length, sentences,  n_iters, print_every=1000, learning_rate=0.01):\n",
        "    print_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_data = [getSentenceTensor(lang_dict, random.choice(sentences))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        iter_data = training_data[iter - 1]\n",
        "        input_tensor = iter_data\n",
        "        target_tensor = iter_data\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('iter, loss =  (%d %.4f) ' %  (iter,  print_loss_avg))\n",
        "\n",
        "\n",
        "\n",
        "lang_dict, sentences, max_length = prepareData()\n",
        "print(max_length)\n",
        "\n",
        "encoder = EncoderRNN(lang_dict.n_words, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, lang_dict.n_words).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, lang_dict, max_length, sentences, 10000, print_every=100)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "iter, loss =  (100 6.1886) \n",
            "iter, loss =  (200 6.9915) \n",
            "iter, loss =  (300 7.0646) \n",
            "iter, loss =  (400 8.2057) \n",
            "iter, loss =  (500 6.9078) \n",
            "iter, loss =  (600 6.1363) \n",
            "iter, loss =  (700 5.8263) \n",
            "iter, loss =  (800 5.9630) \n",
            "iter, loss =  (900 5.7585) \n",
            "iter, loss =  (1000 5.5154) \n",
            "iter, loss =  (1100 5.5166) \n",
            "iter, loss =  (1200 5.7481) \n",
            "iter, loss =  (1300 5.6655) \n",
            "iter, loss =  (1400 5.1700) \n",
            "iter, loss =  (1500 5.4807) \n",
            "iter, loss =  (1600 5.0331) \n",
            "iter, loss =  (1700 6.2550) \n",
            "iter, loss =  (1800 6.3051) \n",
            "iter, loss =  (1900 5.1287) \n",
            "iter, loss =  (2000 5.1800) \n",
            "iter, loss =  (2100 5.6950) \n",
            "iter, loss =  (2200 5.6095) \n",
            "iter, loss =  (2300 5.6670) \n",
            "iter, loss =  (2400 5.3212) \n",
            "iter, loss =  (2500 5.5906) \n",
            "iter, loss =  (2600 5.6900) \n",
            "iter, loss =  (2700 5.9532) \n",
            "iter, loss =  (2800 5.2481) \n",
            "iter, loss =  (2900 5.2526) \n",
            "iter, loss =  (3000 5.9651) \n",
            "iter, loss =  (3100 5.3910) \n",
            "iter, loss =  (3200 5.2651) \n",
            "iter, loss =  (3300 5.2902) \n",
            "iter, loss =  (3400 5.1170) \n",
            "iter, loss =  (3500 5.7715) \n",
            "iter, loss =  (3600 5.0861) \n",
            "iter, loss =  (3700 5.3739) \n",
            "iter, loss =  (3800 5.7716) \n",
            "iter, loss =  (3900 6.1119) \n",
            "iter, loss =  (4000 5.2300) \n",
            "iter, loss =  (4100 5.4385) \n",
            "iter, loss =  (4200 5.8190) \n",
            "iter, loss =  (4300 5.6845) \n",
            "iter, loss =  (4400 5.6297) \n",
            "iter, loss =  (4500 5.3548) \n",
            "iter, loss =  (4600 5.9733) \n",
            "iter, loss =  (4700 5.7583) \n",
            "iter, loss =  (4800 5.5905) \n",
            "iter, loss =  (4900 5.4990) \n",
            "iter, loss =  (5000 5.4337) \n",
            "iter, loss =  (5100 5.5359) \n",
            "iter, loss =  (5200 5.6904) \n",
            "iter, loss =  (5300 5.4815) \n",
            "iter, loss =  (5400 5.8260) \n",
            "iter, loss =  (5500 6.0504) \n",
            "iter, loss =  (5600 5.7257) \n",
            "iter, loss =  (5700 5.7779) \n",
            "iter, loss =  (5800 5.6171) \n",
            "iter, loss =  (5900 5.9637) \n",
            "iter, loss =  (6000 6.1394) \n",
            "iter, loss =  (6100 5.5668) \n",
            "iter, loss =  (6200 5.4724) \n",
            "iter, loss =  (6300 5.4511) \n",
            "iter, loss =  (6400 5.5061) \n",
            "iter, loss =  (6500 5.5742) \n",
            "iter, loss =  (6600 5.7108) \n",
            "iter, loss =  (6700 5.5418) \n",
            "iter, loss =  (6800 5.8497) \n",
            "iter, loss =  (6900 5.7322) \n",
            "iter, loss =  (7000 5.8163) \n",
            "iter, loss =  (7100 5.7585) \n",
            "iter, loss =  (7200 5.9341) \n",
            "iter, loss =  (7300 5.7661) \n",
            "iter, loss =  (7400 5.6004) \n",
            "iter, loss =  (7500 5.6980) \n",
            "iter, loss =  (7600 5.8510) \n",
            "iter, loss =  (7700 5.7360) \n",
            "iter, loss =  (7800 5.6646) \n",
            "iter, loss =  (7900 5.2499) \n",
            "iter, loss =  (8000 5.6619) \n",
            "iter, loss =  (8100 5.3573) \n",
            "iter, loss =  (8200 5.6538) \n",
            "iter, loss =  (8300 5.9601) \n",
            "iter, loss =  (8400 5.3312) \n",
            "iter, loss =  (8500 5.6631) \n",
            "iter, loss =  (8600 5.9106) \n",
            "iter, loss =  (8700 5.5760) \n",
            "iter, loss =  (8800 5.1576) \n",
            "iter, loss =  (8900 5.2929) \n",
            "iter, loss =  (9000 5.1836) \n",
            "iter, loss =  (9100 5.5159) \n",
            "iter, loss =  (9200 5.4972) \n",
            "iter, loss =  (9300 5.7652) \n",
            "iter, loss =  (9400 5.7970) \n",
            "iter, loss =  (9500 5.2588) \n",
            "iter, loss =  (9600 5.6191) \n",
            "iter, loss =  (9700 5.6795) \n",
            "iter, loss =  (9800 5.6147) \n",
            "iter, loss =  (9900 5.1571) \n",
            "iter, loss =  (10000 5.3031) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kQif4-iwIFL"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "MAX_LENGTH = 10  # Maximum sentence length\n",
        "\n",
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "        keep_words = []\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "\n",
        "# Lowercase and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "# Takes string sentence, returns sentence of word indexes\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] if word in voc.word2index else PAD_token for word in sentence.split(' ') ] + [EOS_token]\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # type: (Tensor, Tensor, Optional[Tensor]) -> Tuple[Tensor, Tensor]\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden\n",
        "\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, decoder_n_layers):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self._device = device\n",
        "        self._SOS_token = SOS_token\n",
        "        self._decoder_n_layers = decoder_n_layers\n",
        "\n",
        "    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']\n",
        "\n",
        "    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=self._device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CDwguQu0zla"
      },
      "source": [
        "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "# Evaluate inputs from user input (stdin)\n",
        "def evaluateInput(searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")\n",
        "\n",
        "# Normalize input sentence and call evaluate()\n",
        "def evaluateExample(sentence, searcher, voc):\n",
        "    print(\"> \" + sentence)\n",
        "    # Normalize sentence\n",
        "    input_sentence = normalizeString(sentence)\n",
        "    # Evaluate sentence\n",
        "    output_words = evaluate(searcher, voc, input_sentence)\n",
        "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "    print('Bot:', ' '.join(output_words))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4vw1FBz3TqE"
      },
      "source": [
        "\n",
        "def prepareData():\n",
        "    sentences = []\n",
        "    max_length=0\n",
        "    with open(\"./extracted_dialogue.txt\",\"r\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\")\n",
        "        for i in range(0, len(lines), 2):\n",
        "            tmp = lines[i].strip()\n",
        "            tmp = normalizeString(tmp)\n",
        "            sentences.append(tmp)\n",
        "    return sentences"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lehM2I0IxBQc",
        "outputId": "d09b7d22-cb4c-4249-8bd7-e92cdd1c8701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "corpus_name = \"cornell movie-dialogs corpus\"\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "\n",
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# If you're loading your own model\n",
        "# Set checkpoint to load from\n",
        "checkpoint_iter = 4000\n",
        "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                             '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "# If you're loading the hosted model\n",
        "loadFilename = './4000_checkpoint.tar'\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(loadFilename, map_location=device)\n",
        "encoder_sd = checkpoint['en']\n",
        "decoder_sd = checkpoint['de']\n",
        "encoder_optimizer_sd = checkpoint['en_opt']\n",
        "decoder_optimizer_sd = checkpoint['de_opt']\n",
        "embedding_sd = checkpoint['embedding']\n",
        "voc = Voc(corpus_name)\n",
        "voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "# Load trained model params\n",
        "encoder.load_state_dict(encoder_sd)\n",
        "decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "print('Models built and ready to go!')\n",
        "\n",
        "\n",
        "### Compile the whole greedy search model to TorchScript model\n",
        "# Create artificial inputs\n",
        "test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words).to(device)\n",
        "test_seq_length = torch.LongTensor([test_seq.size()[0]]).to(device)\n",
        "# Trace the model\n",
        "traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n",
        "\n",
        "### Convert decoder model\n",
        "# Create and generate artificial inputs\n",
        "test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\n",
        "test_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\n",
        "test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n",
        "# Trace the model\n",
        "traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n",
        "\n",
        "### Initialize searcher module by wrapping ``torch.jit.script`` call\n",
        "scripted_searcher = torch.jit.script(GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers))\n",
        "\n",
        "scripted_searcher.to(device)\n",
        "# Set dropout layers to eval mode\n",
        "scripted_searcher.eval()\n",
        "\n",
        "# Evaluate examples\n",
        "sentences = prepareData()[:10]\n",
        "print(sentences)\n",
        "for s in sentences:\n",
        "    evaluateExample(s, scripted_searcher, voc)\n",
        "\n",
        "# Evaluate your input\n",
        "#evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/jit/_trace.py:152: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if a.grad is not None:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .', 'well i thought we d start with pronunciation if that s okay with you .', 'not the hacking and gagging and spitting part . please .', 'you re asking me out . that s so cute . what s your name again ?', 'no no it s my fault we didn t have a proper introduction ', 'cameron .', 'the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .', 'why ?', 'unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .', 'gosh if only we could find kat a boyfriend . . .']\n",
            "> can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .\n",
            "Bot: sure .\n",
            "> well i thought we d start with pronunciation if that s okay with you .\n",
            "Bot: i ll take it .\n",
            "> not the hacking and gagging and spitting part . please .\n",
            "Bot: oh yes i m sorry about zeus .\n",
            "> you re asking me out . that s so cute . what s your name again ?\n",
            "Bot: what ?\n",
            "> no no it s my fault we didn t have a proper introduction \n",
            "Bot: i m not afraid of anything .\n",
            "> cameron .\n",
            "Bot: yes ?\n",
            "> the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .\n",
            "Bot: but . . .\n",
            "> why ?\n",
            "Bot: i don t know .\n",
            "> unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .\n",
            "Bot: what ?\n",
            "> gosh if only we could find kat a boyfriend . . .\n",
            "Bot: you re a liar .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}